# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

BM_train = pd.read_csv('H:\\DS\\Python\\BigMartSales\\Train_BM.csv')
BM_test = pd.read_csv('H:\\DS\\Python\\BigMartSales\\Test_BM.csv')

BM_train.isnull().sum()
# Missing columns Item_Weight and OutletSize
BM_test.isnull().sum()
# Missing columns Item_Weight and OutletSize

## Item_Outlet_Sales column is missing from test
BM_train[BM_train["Item_Outlet_Sales"]==0]

# None of sales column is 0 in training dataset lets add sales column in test and  impute 0

BM_test["Item_Outlet_Sales"]=0

BM_train.shape
BM_test.shape

BM_all = pd.concat([BM_train,BM_test],axis=0)
BM_all.reset_index(inplace=True,drop=True)
BM_all.shape

##### lets look some basic statistics of numerical variables
BM_all.describe()
##### FINDINGS
# 1) Item_Visibility is 0 for some of the items that is practically impossible
# 2) Outlet_Establishment_Year is not making sence it was better to have how old store is 2013 - estibleshment year

## Checking nominal(Categorical) valiables

#     unique values for each column

BM_all.apply(lambda x: len(x.unique()))

# FIlter Categorical variables

categorical_columns = [x for x in BM_all.dtypes.index if BM_all.dtypes[x]=='object']
## ignoring IDs from the list
categorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier']]

# Finding frequencey of all the columns

for col in categorical_columns:
    print ('\nFrequency of Categories for varible %s'%col)
    print (BM_all[col].value_counts())

## Here we found
# Item_Fat_Content column has same value written diffrently
## Item_Type column value probably we can coming some item_type to have less no of category

BM_all['Item_Fat_Content'].value_counts()
                  
BM_all['Item_Fat_Content'] = BM_all['Item_Fat_Content'].replace({'LF':'Low Fat','reg':'Regular','low fat':'Low Fat'})


## try to imputing missing values 
BM_all.isnull().sum()

##Counter(BM_all["Item_Weight"])
BM_all.head(2)

#3 Fill Item weight with average of each product type(Item_Identifier)

item_avg_weight = BM_all.pivot_table(values='Item_Weight',index='Item_Identifier')
## OR
item_avg_weight = BM_all.groupby('Item_Identifier')['Item_Weight'].mean()

## getting missing rows as boolean
miss_bool = BM_all['Item_Weight'].isnull()

BM_all.loc[miss_bool,'Item_Weight'] = BM_all.loc[miss_bool,'Item_Identifier'].apply(lambda x :item_avg_weight[x] )

BM_all.isnull().sum()
## no missing value for weight

Counter(BM_all['Outlet_Size'])
Counter(BM_all['Outlet_Type'])

pd.crosstab(BM_all['Outlet_Size'].isnull(),BM_all['Outlet_Type'])
## it shows that 925 missing values are for Grocery Store and 3091 missing values for Supermarket Type1

pd.crosstab(BM_all['Outlet_Type'],BM_all['Outlet_Size'])

# result showa that Grocery stores are always small and around 50% Supermarket Type1 are of small size however 25 % High and 25% Medium

## we can directly put size as Small for 880 missing values of grocery store

BM_all.loc[((BM_all["Outlet_Type"]=='Grocery Store') & (BM_all["Outlet_Size"].isnull())),'Outlet_Size']='Small'
BM_all.loc[((BM_all["Outlet_Type"]=='Supermarket Type1') & (BM_all["Outlet_Size"].isnull())),'Outlet_Size']='Small'

#Item type
Counter(BM_all['Item_Identifier'].apply(lambda x:x[0:2]))
BM_all["Item_Type_Combined"]  = BM_all['Item_Identifier'].apply(lambda x:x[0:2]).map({'DR':'Drinks','FD':'Food','NC':'Non-Consumable'})
Counter(BM_all["Item_Type_Combined"])
BM_all["Item_Type_Combined"].value_counts()

BM_all.loc[(BM_all["Item_Visibility"]==0),'Item_Identifier']

pd.crosstab((BM_all["Item_Visibility"]==0),BM_all['Item_Identifier'])

## we will take average of each item type here and will assign to items where visibility score is 0

item_avg_visibility= BM_all.groupby('Item_Identifier')['Item_Visibility'].mean()
visibility_0 = (BM_all['Item_Visibility']==0)

BM_all.loc[visibility_0,'Item_Visibility'] = BM_all.loc[visibility_0,'Item_Identifier'].apply(lambda x :item_avg_visibility[x] )

sum(BM_all['Item_Visibility']==0)

BM_all.info()

BM_all['Outlet_years'] = 2013 - BM_all['Outlet_Establishment_Year']

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

BM_all['Outlet'] = le.fit_transform(BM_all['Outlet_Identifier'])

var_mod = ['Item_Fat_Content','Outlet_Size','Outlet_Location_Type','Outlet_Type','Item_Type_Combined','Outlet']

le = LabelEncoder()
for i in var_mod:
    BM_all[i] = le.fit_transform(BM_all[i])
    
    
BM_all = pd.get_dummies(BM_all,columns=['Item_Fat_Content','Outlet_Size','Outlet_Location_Type','Outlet_Type'
                                        ,'Item_Type_Combined','Outlet'])
    
    
BM_all.info()

## Droping columns those are re created

BM_all.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)

Bm_train = BM_all[BM_all['Item_Outlet_Sales'] !=0]
Bm_test = BM_all[BM_all['Item_Outlet_Sales'] ==0]

#3 from test drop sales columns
Bm_test.drop(['Item_Outlet_Sales'],axis=1,inplace=True)







#### @@@@@@@@@@@@@@@@@@ MODEL BUILDING @@@@@@@@@@@@@@@@@@@@@@ ######################

  ## Creating a generic function to run againt different models
#Define target and ID columns:
target = 'Item_Outlet_Sales'
IDcol = ['Item_Identifier','Outlet_Identifier']
from sklearn import cross_validation, metrics
import statsmodels.api as sm
def modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):
    #Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain[target])
    X= sm.add_constant(dtrain[predictors])
    Y= dtrain[target]
    a1=sm.OLS(Y,X)
    a=a1.fit()
    print (a.summary())
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])

    #Perform cross-validation:
    cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], cv=20, scoring='mean_squared_error')
    cv_score = np.sqrt(np.abs(cv_score))
    
    #Print model report:
    print ("\nModel Report")
    print ("RMSE : %.4g" % np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))
    print ("CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))
    
    #Predict on testing data:
    dtest[target] = alg.predict(dtest[predictors])
    
    #Export submission file:
    IDcol.append(target)
    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})
    submission.to_csv(filename, index=False)


import statsmodels.stats.weightstats

sm.datasets.get_rdataset("Guerry", "HistData").data

from sklearn.linear_model import LinearRegression,Ridge,Lasso

predictors = [x for x in Bm_train.columns if x not in [target]+IDcol]
alg1 = LinearRegression(normalize=True)
modelfit(alg1,Bm_train,Bm_test,predictors,target,IDcol,'H:\\DS\\Python\\BigMartSales\\alg1.csv')

coef1 = pd.Series(alg1.coef_,predictors).sort_values()
coef1.plot(kind='bar',title='Model Coefficients')

## Ridge Regression Model:

predictors = [x for x in Bm_train.columns if x not in [target]+IDcol]
alg2 = Ridge(alpha=0.05,normalize=True)
modelfit(alg2, Bm_train, Bm_test, predictors, target, IDcol, 'H:\\DS\\Python\\BigMartSales\\alg2.csv')
coef2 = pd.Series(alg2.coef_, predictors).sort_values()
coef2.plot(kind='bar', title='Model Coefficients')

## Decision Tree Model


from sklearn.tree import DecisionTreeRegressor
predictors = [x for x in Bm_train.columns if x not in [target]+IDcol]
alg3 = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)
modelfit(alg3, Bm_train, Bm_test, predictors, target, IDcol, 'H:\\DS\\Python\\BigMartSales\\alg3.csv')
coef3 = pd.Series(alg3.feature_importances_, predictors).sort_values(ascending=False)
coef3.plot(kind='bar', title='Feature Importances')

predictors = ['Item_MRP','Outlet_Type_0','Outlet_years','Item_Visibility','Item_Weight']
alg4 = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)
modelfit(alg4, Bm_train, Bm_test, predictors, target, IDcol, 'H:\\DS\\Python\\BigMartSales\\alg4.csv')
coef4 = pd.Series(alg4.feature_importances_, predictors).sort_values(ascending=False)
coef4.plot(kind='bar', title='Feature Importances')

##Random Forest Model

from sklearn.ensemble import RandomForestRegressor
predictors = [x for x in Bm_train.columns if x not in [target]+IDcol]
alg5 = RandomForestRegressor(n_estimators=200,max_depth=5, min_samples_leaf=100,n_jobs=4)
modelfit(alg5, Bm_train, Bm_test, predictors, target, IDcol, 'H:\\DS\\Python\\BigMartSales\\alg5.csv')
coef5 = pd.Series(alg5.feature_importances_, predictors).sort_values(ascending=False)
coef5.plot(kind='bar', title='Feature Importances')

predictors = [x for x in Bm_train.columns if x not in [target]+IDcol]
alg6 = RandomForestRegressor(n_estimators=400,max_depth=6, min_samples_leaf=100,n_jobs=4)
modelfit(alg6, Bm_train, Bm_test, predictors, target, IDcol, 'H:\\DS\\Python\\BigMartSales\\alg6.csv')
coef6 = pd.Series(alg6.feature_importances_, predictors).sort_values(ascending=False)
coef6.plot(kind='bar', title='Feature Importances')
